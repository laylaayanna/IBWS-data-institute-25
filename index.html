# What we learned at the Data Institute
<p class="intro">Here's what our class was taught during the two-week <a href="https://cjddatainstitute.org/">Data Institute</a> from the<a href="https://idabwellssociety.org/"> Ida B. Wells Society for Investigative Reporting</a> and the <a href="https://cfjd.howard.edu/"> Center for Journalism and Democracy</a>. The institute was divided into several categories including an introduction to data reporting, visualization and design, and web scraping, but all of it was geared toward one idea: 
<strong>Exposure to tools and best practices for gathering and thoroughly analyzing data for investigative reporting.</strong></p>
<h2>The tools
</h2>
<dl class="get_data">
<h3>Tabula
</h3>
<dt>For extracting data from<a href="https://tabula.technology/">
readable PDFs</a> 
</dt>
<dd>We were taught to use a free tool called Tabula, which allows you to grab charts and convert them to .csv files for analysis. Here's an example of what that could look like: 
</dd><img width="600" height="300" alt="Screenshot 2025-07-18 123001" src="https://github.com/user-attachments/assets/f583374a-7edd-421e-a0a5-838b249cdd41" />
  <h3>LLMs</h3>
  <dt>Using LLMs to extract data</dt>
  <dd>We learned how to create and use prompts with LLM for data extraction. This elimiates the risk of the LLM trying to create its own data. That's a no.</dd>
  <dd>One use case? Turning notes taken in a .doc format into a .csv with the correct columns, rows and cell entries.</dd>
  <h3>Data diaries</h3>
<dt>Writing down what you've done, every step of the way
</dt>
  <dd>Data diaries are way to track and share methodology.</dd>
  <dd>They can be published to boost story, publication and writer credibility.</dd>

<h3>Tips on filing FOIA requests</h3>
<dd>Automatic request generators like
<a href="https://splc.org/lettergenerator/">this one from the Student Press Law Center
</a> generate FOIA letters.
<a href="https://www.foiamachine.org/"> This one from MuckRock
</a>can also track the progress of your requests. Both tools are free.</dd>
</dl>
<dl class="clean_data">
<h3>Google Sheets
</h3>
<dt>For formatting and analyzing the data
</dt>
<dd>We learned to use filters, pivot tables and charts (even with a little statistics) to find patterns, outliers and discrepancies.
</dd>
<dt>Other platforms we used to help analyze data</dt>
  <ul>
    <li><a href="https://propublica.s3.amazonaws.com/data-institute/open-refine-2025.pdf">Open Refine
<a></li>
    <li>Google Colab</li>
  </ul>
  <h3>And, of course, we did some math</h3>
  <dt>Just a little
  <dd>We learned the percent change formula:
  <ul>
    <li>(new-old)/old</li>
  </ul>
  And the per capita formula:
  <ul>
    <li>what you’re measuring/a control[population]*a normalizer (per 100k ppl for example)</li>
  </ul></dd></dt>
</dl>
  <dl class="coding">
<h2>Coding and visualizing data</h2>
  <h3>Coding languages</h3>
<dt>There are many of them!
</dt>
<dd>All that really matters is most journalists use Python and R, which is helpful for trouble shooting
  <dd>We learned basic HTML and CSS. That's how I made this page on GitHub.</dd>
  <dd>We used local and web-based platforms that let you write and edit code to correspond with a website. These platforms include:</dd>
  <ul>
    <li>GitHub</li>
    <li>Sublime Text</li>
    <li><a href="https://colab.research.google.com/">Google Colab
</a></li>
  </ul>
<h3>Webscraping</h3>
  <dt>We learned to use pre-coded webscrapers to automate gathering large datasets.</dt>
  <dd>Tools we used included: 
    <ul></ul>
    <li><a href="https://chromewebstore.google.com/detail/instant-data-scraper/ofaokhiedipichpaobibbnahnkdoiiah?hl=en-US">Instant Data Scraper
</a>, a basic Google Chrome plug-in,</li>
    <li><a href="https://parsehub.com/">Parsehub
</a>, and</li>
    <li><a href="https://distill.io/">Distill.io</a> ← This one was my favorite. It allows you to track and get email updates about changes to a website</li>
  </dd>
  <dt>Ultimately, the biggest takeaway was:</dt>
  <dd>there are several pre-coded tools you can use, without having to do any coding yourself.</dd>
  </dl>
<dl class="viz_data">
  <h2>Visualizing data</h2>
<dt>We built charts on <a href="https://www.datawrapper.de/">Datawrapper
</a>
</dt>
<dd>This was more of a refresher for me. I'd done it at Billy Penn, but also learned new tips to help improve the process, like how to make charts mobile-friendly.
</dd>
<dt>Mapping on Tableau
</dt>
<dd>
<a href="https://www.tableau.com/trial/visualize-your-data">Tableau
</a> lets you upload .csv's with locality information, and maps it.
</dd>
<dl class="instructors">
  <h2>Who taught us</h2>
  <dt>Some of our instructors include</dt>
  <ul class="instructors">
    <li><a href="https://github.com/ellissimani">Ellis Imani
</a> of ProPublica</li>
    <li><a href="https://thescoop.org/about/">Derek Willis</a> of the University of Maryland J School</li>
    <li><a href="http://maggielee.net/">Maggie Lee
</a>, a data reporter at The Courant in Georgia</li>
    <li><a href="https://sisiwei.com/">Sisi Wei
</a>, who co-founded the Institute</li>
  </ul></dl>
<h4>Thanks so much for allowing me to go to this program, especially so soon after starting this position. I look forward to using what I learned to improve my journalism at TPM!
</h4>
